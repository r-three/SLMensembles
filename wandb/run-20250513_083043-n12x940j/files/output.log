Truncating train dataset: 100%|█████████████| 192688/192688 [00:19<00:00, 9665.44 examples/s]
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|                                                               | 0/1000 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/h/klambert/slm_ensembles/train.py", line 295, in <module>
    trainer.train()
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/transformers/trainer.py", line 3736, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/h/klambert/slm_ensembles/train.py", line 155, in compute_loss
    student_logits = model(input_ids, attention_mask=attention_mask).logits
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 814, in forward
    return model_forward(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/accelerate/utils/operations.py", line 802, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/accelerate/hooks.py", line 176, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/transformers/utils/generic.py", line 965, in wrapper
    output = func(self, *args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 823, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/transformers/utils/generic.py", line 965, in wrapper
    output = func(self, *args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 519, in forward
    causal_mask = self._update_causal_mask(
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 613, in _update_causal_mask
    if AttentionMaskConverter._ignore_causal_mask_sdpa(
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/env/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py", line 288, in _ignore_causal_mask_sdpa
    elif not is_tracing and torch.all(attention_mask == 1):
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
