Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.

==================================================
Starting Round 0 at: 2025-05-29 17:12:50
==================================================
Round '0' model stored in: /scratch/ssd004/scratch/klambert/slm_ensembles/boosted_distillation_1.5B_teacher_average_fixed_logging/2025-05-29/run_5/round_0
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
[34m[1mwandb[0m: [33mWARNING[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.
                                                             
                                                             
{'eval_loss': 0.9272032976150513, 'eval_runtime': 52.9284, 'eval_samples_per_second': 36.767, 'eval_steps_per_second': 4.61, 'epoch': 0}
[DEBUG] Unmasked tokens this batch: 1419
[DEBUG] Unmasked tokens this batch: 1089
[DEBUG] Unmasked tokens this batch: 2388
[DEBUG] Unmasked tokens this batch: 1085
[DEBUG] Unmasked tokens this batch: 1331
[DEBUG] Unmasked tokens this batch: 1607
[DEBUG] Unmasked tokens this batch: 394
[DEBUG] Unmasked tokens this batch: 1869
{'loss': 2.0805, 'grad_norm': 23.125, 'learning_rate': 0.0, 'epoch': 0.0}
{'eval_loss': 0.9272032976150513, 'eval_runtime': 52.2067, 'eval_samples_per_second': 37.275, 'eval_steps_per_second': 4.674, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 1500
[DEBUG] Unmasked tokens this batch: 2024
[DEBUG] Unmasked tokens this batch: 1162
[DEBUG] Unmasked tokens this batch: 1120
[DEBUG] Unmasked tokens this batch: 1493
[DEBUG] Unmasked tokens this batch: 1112
[DEBUG] Unmasked tokens this batch: 679
[DEBUG] Unmasked tokens this batch: 1297
{'loss': 1.7785, 'grad_norm': 21.0, 'learning_rate': 2e-08, 'epoch': 0.0}
{'eval_loss': 0.9271343350410461, 'eval_runtime': 52.2226, 'eval_samples_per_second': 37.264, 'eval_steps_per_second': 4.672, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 1303
[DEBUG] Unmasked tokens this batch: 1035
[DEBUG] Unmasked tokens this batch: 1515
[DEBUG] Unmasked tokens this batch: 862
[DEBUG] Unmasked tokens this batch: 1901
[DEBUG] Unmasked tokens this batch: 1191
[DEBUG] Unmasked tokens this batch: 1092
[DEBUG] Unmasked tokens this batch: 1447
{'loss': 2.1342, 'grad_norm': 25.375, 'learning_rate': 4e-08, 'epoch': 0.0}
{'eval_loss': 0.9270668625831604, 'eval_runtime': 52.1066, 'eval_samples_per_second': 37.347, 'eval_steps_per_second': 4.683, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 1420
[DEBUG] Unmasked tokens this batch: 1528
[DEBUG] Unmasked tokens this batch: 2168
[DEBUG] Unmasked tokens this batch: 857
[DEBUG] Unmasked tokens this batch: 1397
[DEBUG] Unmasked tokens this batch: 1265
[DEBUG] Unmasked tokens this batch: 1016
[DEBUG] Unmasked tokens this batch: 1412
{'loss': 1.5334, 'grad_norm': 20.5, 'learning_rate': 6e-08, 'epoch': 0.0}
{'eval_loss': 0.9273866415023804, 'eval_runtime': 52.0774, 'eval_samples_per_second': 37.367, 'eval_steps_per_second': 4.685, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 2229
[DEBUG] Unmasked tokens this batch: 1515
[DEBUG] Unmasked tokens this batch: 2001
[DEBUG] Unmasked tokens this batch: 903
[DEBUG] Unmasked tokens this batch: 1356
[DEBUG] Unmasked tokens this batch: 342
[DEBUG] Unmasked tokens this batch: 2633
[DEBUG] Unmasked tokens this batch: 1834
{'loss': 2.1318, 'grad_norm': 29.5, 'learning_rate': 8e-08, 'epoch': 0.0}
{'eval_loss': 0.9271442294120789, 'eval_runtime': 51.9954, 'eval_samples_per_second': 37.426, 'eval_steps_per_second': 4.693, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 1752
[DEBUG] Unmasked tokens this batch: 1794
[DEBUG] Unmasked tokens this batch: 1026
[DEBUG] Unmasked tokens this batch: 1255
[DEBUG] Unmasked tokens this batch: 589
[DEBUG] Unmasked tokens this batch: 1901
[DEBUG] Unmasked tokens this batch: 1503
[DEBUG] Unmasked tokens this batch: 1688
{'loss': 1.3309, 'grad_norm': 21.75, 'learning_rate': 1e-07, 'epoch': 0.0}
{'eval_loss': 0.927232027053833, 'eval_runtime': 51.9376, 'eval_samples_per_second': 37.468, 'eval_steps_per_second': 4.698, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 852
[DEBUG] Unmasked tokens this batch: 780
[DEBUG] Unmasked tokens this batch: 1485
[DEBUG] Unmasked tokens this batch: 2083
[DEBUG] Unmasked tokens this batch: 829
[DEBUG] Unmasked tokens this batch: 1539
[DEBUG] Unmasked tokens this batch: 778
[DEBUG] Unmasked tokens this batch: 1541
{'loss': 1.7089, 'grad_norm': 18.875, 'learning_rate': 1.2e-07, 'epoch': 0.0}
{'eval_loss': 0.927286684513092, 'eval_runtime': 51.934, 'eval_samples_per_second': 37.471, 'eval_steps_per_second': 4.698, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 1183
[DEBUG] Unmasked tokens this batch: 1001
[DEBUG] Unmasked tokens this batch: 1790
[DEBUG] Unmasked tokens this batch: 1523
[DEBUG] Unmasked tokens this batch: 1049
[DEBUG] Unmasked tokens this batch: 1826
[DEBUG] Unmasked tokens this batch: 625
[DEBUG] Unmasked tokens this batch: 2284
{'loss': 1.9149, 'grad_norm': 25.375, 'learning_rate': 1.4e-07, 'epoch': 0.0}
{'eval_loss': 0.9274996519088745, 'eval_runtime': 51.9266, 'eval_samples_per_second': 37.476, 'eval_steps_per_second': 4.699, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 970
[DEBUG] Unmasked tokens this batch: 1055
[DEBUG] Unmasked tokens this batch: 1634
[DEBUG] Unmasked tokens this batch: 1800
[DEBUG] Unmasked tokens this batch: 1361
[DEBUG] Unmasked tokens this batch: 1230
[DEBUG] Unmasked tokens this batch: 1579
[DEBUG] Unmasked tokens this batch: 1785
{'loss': 1.2979, 'grad_norm': 17.625, 'learning_rate': 1.6e-07, 'epoch': 0.0}
{'eval_loss': 0.9270925521850586, 'eval_runtime': 51.9161, 'eval_samples_per_second': 37.484, 'eval_steps_per_second': 4.7, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 1415
[DEBUG] Unmasked tokens this batch: 1813
[DEBUG] Unmasked tokens this batch: 1198
[DEBUG] Unmasked tokens this batch: 1943
[DEBUG] Unmasked tokens this batch: 981
[DEBUG] Unmasked tokens this batch: 1026
[DEBUG] Unmasked tokens this batch: 1179
[DEBUG] Unmasked tokens this batch: 1326
{'loss': 1.3077, 'grad_norm': 22.375, 'learning_rate': 1.8e-07, 'epoch': 0.0}
{'eval_loss': 0.9273368716239929, 'eval_runtime': 51.9232, 'eval_samples_per_second': 37.478, 'eval_steps_per_second': 4.699, 'epoch': 0.0}
{'train_runtime': 640.7532, 'train_samples_per_second': 0.499, 'train_steps_per_second': 0.016, 'train_loss': 1.7218771815299987, 'epoch': 0.0}
==================================================
Ending Round 0 at: 2025-05-29 17:24:23
Completed in: 11m 32s
Total training time: 11m 40s
==================================================


==================================================
Starting Round 1 at: 2025-05-29 17:24:23
==================================================
Round '1' model stored in: /scratch/ssd004/scratch/klambert/slm_ensembles/boosted_distillation_1.5B_teacher_average_fixed_logging/2025-05-29/run_5/round_1
Truncating train dataset: 100%|â–ˆ| 192701/192701 [00:24<00:00,
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
                                                             
                                                             
{'eval_loss': 0.9271153807640076, 'eval_runtime': 103.3041, 'eval_samples_per_second': 18.838, 'eval_steps_per_second': 2.362, 'epoch': 0}
[DEBUG] Unmasked tokens this batch: 1120
[DEBUG] Unmasked tokens this batch: 1511
[DEBUG] Unmasked tokens this batch: 590
[DEBUG] Unmasked tokens this batch: 2210
[DEBUG] Unmasked tokens this batch: 1506
[DEBUG] Unmasked tokens this batch: 1689
[DEBUG] Unmasked tokens this batch: 1713
[DEBUG] Unmasked tokens this batch: 1111
{'loss': 2.144, 'grad_norm': 11.8125, 'learning_rate': 0.0, 'epoch': 0.0}
{'eval_loss': 0.9271153807640076, 'eval_runtime': 103.2002, 'eval_samples_per_second': 18.857, 'eval_steps_per_second': 2.364, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 1664
[DEBUG] Unmasked tokens this batch: 1566
[DEBUG] Unmasked tokens this batch: 1180
[DEBUG] Unmasked tokens this batch: 586
[DEBUG] Unmasked tokens this batch: 2217
[DEBUG] Unmasked tokens this batch: 246
[DEBUG] Unmasked tokens this batch: 2032
[DEBUG] Unmasked tokens this batch: 1591
{'loss': 2.1689, 'grad_norm': 13.4375, 'learning_rate': 2e-08, 'epoch': 0.0}
{'eval_loss': 0.927090048789978, 'eval_runtime': 103.0529, 'eval_samples_per_second': 18.884, 'eval_steps_per_second': 2.368, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 2237
[DEBUG] Unmasked tokens this batch: 1203
[DEBUG] Unmasked tokens this batch: 1508
[DEBUG] Unmasked tokens this batch: 1851
[DEBUG] Unmasked tokens this batch: 938
[DEBUG] Unmasked tokens this batch: 1760
[DEBUG] Unmasked tokens this batch: 1518
[DEBUG] Unmasked tokens this batch: 454
{'loss': 2.3091, 'grad_norm': 13.125, 'learning_rate': 4e-08, 'epoch': 0.0}
{'eval_loss': 0.9272471070289612, 'eval_runtime': 102.9688, 'eval_samples_per_second': 18.899, 'eval_steps_per_second': 2.37, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 1350
[DEBUG] Unmasked tokens this batch: 1764
[DEBUG] Unmasked tokens this batch: 902
[DEBUG] Unmasked tokens this batch: 1932
[DEBUG] Unmasked tokens this batch: 947
[DEBUG] Unmasked tokens this batch: 342
[DEBUG] Unmasked tokens this batch: 875
[DEBUG] Unmasked tokens this batch: 1666
{'loss': 2.1508, 'grad_norm': 18.25, 'learning_rate': 6e-08, 'epoch': 0.0}
{'eval_loss': 0.9271823763847351, 'eval_runtime': 102.747, 'eval_samples_per_second': 18.94, 'eval_steps_per_second': 2.375, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 635
[DEBUG] Unmasked tokens this batch: 1766
[DEBUG] Unmasked tokens this batch: 739
[DEBUG] Unmasked tokens this batch: 1520
[DEBUG] Unmasked tokens this batch: 1764
[DEBUG] Unmasked tokens this batch: 1765
[DEBUG] Unmasked tokens this batch: 744
[DEBUG] Unmasked tokens this batch: 1030
{'loss': 1.6728, 'grad_norm': 11.625, 'learning_rate': 8e-08, 'epoch': 0.0}
{'eval_loss': 0.9270244240760803, 'eval_runtime': 102.723, 'eval_samples_per_second': 18.944, 'eval_steps_per_second': 2.375, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 1087
[DEBUG] Unmasked tokens this batch: 2091
[DEBUG] Unmasked tokens this batch: 1686
[DEBUG] Unmasked tokens this batch: 1438
[DEBUG] Unmasked tokens this batch: 658
[DEBUG] Unmasked tokens this batch: 1437
[DEBUG] Unmasked tokens this batch: 2413
[DEBUG] Unmasked tokens this batch: 1433
{'loss': 1.2486, 'grad_norm': 10.0625, 'learning_rate': 1e-07, 'epoch': 0.0}
{'eval_loss': 0.9271319508552551, 'eval_runtime': 102.7322, 'eval_samples_per_second': 18.942, 'eval_steps_per_second': 2.375, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 387
[DEBUG] Unmasked tokens this batch: 3060
[DEBUG] Unmasked tokens this batch: 764
[DEBUG] Unmasked tokens this batch: 1492
[DEBUG] Unmasked tokens this batch: 930
[DEBUG] Unmasked tokens this batch: 1552
[DEBUG] Unmasked tokens this batch: 857
[DEBUG] Unmasked tokens this batch: 2050
{'loss': 1.7944, 'grad_norm': 14.25, 'learning_rate': 1.2e-07, 'epoch': 0.0}
{'eval_loss': 0.9271553754806519, 'eval_runtime': 102.7527, 'eval_samples_per_second': 18.939, 'eval_steps_per_second': 2.375, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 2482
[DEBUG] Unmasked tokens this batch: 721
[DEBUG] Unmasked tokens this batch: 1921
[DEBUG] Unmasked tokens this batch: 794
[DEBUG] Unmasked tokens this batch: 1287
[DEBUG] Unmasked tokens this batch: 642
[DEBUG] Unmasked tokens this batch: 1493
[DEBUG] Unmasked tokens this batch: 907
{'loss': 1.9945, 'grad_norm': 10.5625, 'learning_rate': 1.4e-07, 'epoch': 0.0}
{'eval_loss': 0.9272218942642212, 'eval_runtime': 102.7487, 'eval_samples_per_second': 18.939, 'eval_steps_per_second': 2.375, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 979
[DEBUG] Unmasked tokens this batch: 1795
[DEBUG] Unmasked tokens this batch: 1315
[DEBUG] Unmasked tokens this batch: 1684
[DEBUG] Unmasked tokens this batch: 777
[DEBUG] Unmasked tokens this batch: 1867
[DEBUG] Unmasked tokens this batch: 1371
[DEBUG] Unmasked tokens this batch: 1849
{'loss': 1.6815, 'grad_norm': 9.25, 'learning_rate': 1.6e-07, 'epoch': 0.0}
{'eval_loss': 0.9271327257156372, 'eval_runtime': 102.7286, 'eval_samples_per_second': 18.943, 'eval_steps_per_second': 2.375, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 676
[DEBUG] Unmasked tokens this batch: 1623
[DEBUG] Unmasked tokens this batch: 1505
[DEBUG] Unmasked tokens this batch: 1323
[DEBUG] Unmasked tokens this batch: 1519
[DEBUG] Unmasked tokens this batch: 851
[DEBUG] Unmasked tokens this batch: 1187
[DEBUG] Unmasked tokens this batch: 1856
{'loss': 1.7603, 'grad_norm': 10.0625, 'learning_rate': 1.8e-07, 'epoch': 0.0}
{'eval_loss': 0.927029550075531, 'eval_runtime': 102.7899, 'eval_samples_per_second': 18.932, 'eval_steps_per_second': 2.374, 'epoch': 0.0}
{'train_runtime': 1210.0426, 'train_samples_per_second': 0.264, 'train_steps_per_second': 0.008, 'train_loss': 1.8924748182296753, 'epoch': 0.0}
==================================================
Ending Round 1 at: 2025-05-29 17:45:47
Completed in: 21m 23s
Total training time: 33m 4s
==================================================


==================================================
Starting Round 2 at: 2025-05-29 17:45:47
==================================================
Round '2' model stored in: /scratch/ssd004/scratch/klambert/slm_ensembles/boosted_distillation_1.5B_teacher_average_fixed_logging/2025-05-29/run_5/round_2
Truncating train dataset: 100%|â–ˆ| 192701/192701 [00:25<00:00,
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
                                                             
                                                             
{'eval_loss': 0.9271403551101685, 'eval_runtime': 143.5898, 'eval_samples_per_second': 13.552, 'eval_steps_per_second': 1.699, 'epoch': 0}
[DEBUG] Unmasked tokens this batch: 2001
[DEBUG] Unmasked tokens this batch: 2214
[DEBUG] Unmasked tokens this batch: 1155
[DEBUG] Unmasked tokens this batch: 1789
[DEBUG] Unmasked tokens this batch: 2206
[DEBUG] Unmasked tokens this batch: 962
[DEBUG] Unmasked tokens this batch: 553
[DEBUG] Unmasked tokens this batch: 1037
{'loss': 1.8344, 'grad_norm': 6.875, 'learning_rate': 0.0, 'epoch': 0.0}
{'eval_loss': 0.9271403551101685, 'eval_runtime': 143.3059, 'eval_samples_per_second': 13.579, 'eval_steps_per_second': 1.703, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 2412
[DEBUG] Unmasked tokens this batch: 1984
[DEBUG] Unmasked tokens this batch: 742
[DEBUG] Unmasked tokens this batch: 662
[DEBUG] Unmasked tokens this batch: 589
[DEBUG] Unmasked tokens this batch: 1196
[DEBUG] Unmasked tokens this batch: 1553
[DEBUG] Unmasked tokens this batch: 1743
{'loss': 1.8692, 'grad_norm': 8.125, 'learning_rate': 2e-08, 'epoch': 0.0}
{'eval_loss': 0.9271677732467651, 'eval_runtime': 142.9462, 'eval_samples_per_second': 13.614, 'eval_steps_per_second': 1.707, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 1807
[DEBUG] Unmasked tokens this batch: 1371
[DEBUG] Unmasked tokens this batch: 1178
[DEBUG] Unmasked tokens this batch: 1932
[DEBUG] Unmasked tokens this batch: 1477
[DEBUG] Unmasked tokens this batch: 2046
[DEBUG] Unmasked tokens this batch: 1049
[DEBUG] Unmasked tokens this batch: 1453
{'loss': 2.2976, 'grad_norm': 8.625, 'learning_rate': 4e-08, 'epoch': 0.0}
{'eval_loss': 0.9271674156188965, 'eval_runtime': 142.8268, 'eval_samples_per_second': 13.625, 'eval_steps_per_second': 1.708, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 222
[DEBUG] Unmasked tokens this batch: 1077
[DEBUG] Unmasked tokens this batch: 1362
[DEBUG] Unmasked tokens this batch: 2211
[DEBUG] Unmasked tokens this batch: 1831
[DEBUG] Unmasked tokens this batch: 671
[DEBUG] Unmasked tokens this batch: 1257
[DEBUG] Unmasked tokens this batch: 1512
{'loss': 1.7843, 'grad_norm': 8.4375, 'learning_rate': 6e-08, 'epoch': 0.0}
{'eval_loss': 0.9272435307502747, 'eval_runtime': 142.8724, 'eval_samples_per_second': 13.621, 'eval_steps_per_second': 1.708, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 696
[DEBUG] Unmasked tokens this batch: 140
[DEBUG] Unmasked tokens this batch: 1610
[DEBUG] Unmasked tokens this batch: 1417
[DEBUG] Unmasked tokens this batch: 2932
[DEBUG] Unmasked tokens this batch: 1232
[DEBUG] Unmasked tokens this batch: 893
[DEBUG] Unmasked tokens this batch: 325
{'loss': 2.5473, 'grad_norm': 13.6875, 'learning_rate': 8e-08, 'epoch': 0.0}
{'eval_loss': 0.9271827936172485, 'eval_runtime': 142.804, 'eval_samples_per_second': 13.627, 'eval_steps_per_second': 1.709, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 847
[DEBUG] Unmasked tokens this batch: 2045
[DEBUG] Unmasked tokens this batch: 486
[DEBUG] Unmasked tokens this batch: 2324
[DEBUG] Unmasked tokens this batch: 1634
[DEBUG] Unmasked tokens this batch: 1605
[DEBUG] Unmasked tokens this batch: 404
[DEBUG] Unmasked tokens this batch: 1697
{'loss': 1.8157, 'grad_norm': 7.65625, 'learning_rate': 1e-07, 'epoch': 0.0}
{'eval_loss': 0.9271537661552429, 'eval_runtime': 142.8256, 'eval_samples_per_second': 13.625, 'eval_steps_per_second': 1.708, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 366
[DEBUG] Unmasked tokens this batch: 720
[DEBUG] Unmasked tokens this batch: 1841
[DEBUG] Unmasked tokens this batch: 850
[DEBUG] Unmasked tokens this batch: 659
[DEBUG] Unmasked tokens this batch: 1180
[DEBUG] Unmasked tokens this batch: 860
[DEBUG] Unmasked tokens this batch: 1484
{'loss': 1.9455, 'grad_norm': 9.4375, 'learning_rate': 1.2e-07, 'epoch': 0.0}
{'eval_loss': 0.927179753780365, 'eval_runtime': 142.7294, 'eval_samples_per_second': 13.634, 'eval_steps_per_second': 1.71, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 1737
[DEBUG] Unmasked tokens this batch: 1981
[DEBUG] Unmasked tokens this batch: 911
[DEBUG] Unmasked tokens this batch: 1107
[DEBUG] Unmasked tokens this batch: 893
[DEBUG] Unmasked tokens this batch: 1241
[DEBUG] Unmasked tokens this batch: 1209
[DEBUG] Unmasked tokens this batch: 1242
{'loss': 1.8748, 'grad_norm': 6.5, 'learning_rate': 1.4e-07, 'epoch': 0.0}
{'eval_loss': 0.927099883556366, 'eval_runtime': 142.7785, 'eval_samples_per_second': 13.63, 'eval_steps_per_second': 1.709, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 847
[DEBUG] Unmasked tokens this batch: 1273
[DEBUG] Unmasked tokens this batch: 992
[DEBUG] Unmasked tokens this batch: 2043
[DEBUG] Unmasked tokens this batch: 523
[DEBUG] Unmasked tokens this batch: 888
[DEBUG] Unmasked tokens this batch: 776
[DEBUG] Unmasked tokens this batch: 1490
{'loss': 1.4532, 'grad_norm': 7.78125, 'learning_rate': 1.6e-07, 'epoch': 0.0}
{'eval_loss': 0.9271241426467896, 'eval_runtime': 142.8218, 'eval_samples_per_second': 13.625, 'eval_steps_per_second': 1.708, 'epoch': 0.0}
[DEBUG] Unmasked tokens this batch: 1657
[DEBUG] Unmasked tokens this batch: 1606
[DEBUG] Unmasked tokens this batch: 1842
[DEBUG] Unmasked tokens this batch: 963
[DEBUG] Unmasked tokens this batch: 1270
[DEBUG] Unmasked tokens this batch: 1480
[DEBUG] Unmasked tokens this batch: 1015
[DEBUG] Unmasked tokens this batch: 2026
{'loss': 1.434, 'grad_norm': 6.25, 'learning_rate': 1.8e-07, 'epoch': 0.0}
{'eval_loss': 0.9271421432495117, 'eval_runtime': 142.8174, 'eval_samples_per_second': 13.626, 'eval_steps_per_second': 1.708, 'epoch': 0.0}
{'train_runtime': 1658.5453, 'train_samples_per_second': 0.193, 'train_steps_per_second': 0.006, 'train_loss': 1.8856128454208374, 'epoch': 0.0}
==================================================
Ending Round 2 at: 2025-05-29 18:14:40
Completed in: 28m 52s
Total training time: 61m 57s
==================================================


==================================================
Starting Round 3 at: 2025-05-29 18:14:40
==================================================
Round '3' model stored in: /scratch/ssd004/scratch/klambert/slm_ensembles/boosted_distillation_1.5B_teacher_average_fixed_logging/2025-05-29/run_5/round_3
Truncating train dataset: 100%|â–ˆ| 192701/192701 [00:14<00:00,
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|                                 | 0/10 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/h/klambert/slm_ensembles/collator_train.py", line 394, in <module>
    main()
  File "/h/klambert/slm_ensembles/collator_train.py", line 335, in main
    trainer.train()
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/trainer.py", line 2472, in _inner_training_loop
    self._evaluate(trial, ignore_keys_for_eval, skip_scheduler=True)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/trainer.py", line 3045, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/trainer.py", line 4154, in evaluate
    output = eval_loop(
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/trainer.py", line 4348, in evaluation_loop
    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/h/klambert/slm_ensembles/collator_train.py", line 144, in prediction_step
    ensemble_logits = ensemble_model(input_ids=input_ids, attention_mask=attention_mask).logits.detach()
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/fs01/home/klambert/slm_ensembles/ensemble.py", line 47, in forward
    outputs = model(input_ids.to(device), attention_mask=attention_mask.to(device), **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/utils/generic.py", line 965, in wrapper
    output = func(self, *args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 823, in forward
    outputs: BaseModelOutputWithPast = self.model(
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/utils/generic.py", line 965, in wrapper
    output = func(self, *args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 549, in forward
    layer_outputs = decoder_layer(
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 259, in forward
    hidden_states = self.input_layernorm(hidden_states)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
KeyboardInterrupt
