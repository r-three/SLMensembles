Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.

==================================================
Starting Round 0 at: 2025-05-31 19:15:23
==================================================
Round '0' model stored in: /projects/distilling_llms/model_log/2025-05-31/run_1/round_0
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
[34m[1mwandb[0m: [33mWARNING[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [32:57<00:00, 180.24s/it]Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [33:40<00:00, 202.05s/it]
{'eval_loss': 0.9272032976150513, 'eval_runtime': 174.3682, 'eval_samples_per_second': 11.16, 'eval_steps_per_second': 1.399, 'epoch': 0}
{'loss': 2.0805, 'grad_norm': 23.125, 'learning_rate': 0.0, 'epoch': 0.0}
{'eval_loss': 0.9272032976150513, 'eval_runtime': 175.7429, 'eval_samples_per_second': 11.073, 'eval_steps_per_second': 1.388, 'epoch': 0.0}
{'loss': 1.7785, 'grad_norm': 21.0, 'learning_rate': 2e-08, 'epoch': 0.0}
{'eval_loss': 0.9271876215934753, 'eval_runtime': 175.6174, 'eval_samples_per_second': 11.081, 'eval_steps_per_second': 1.389, 'epoch': 0.0}
{'loss': 2.1328, 'grad_norm': 25.375, 'learning_rate': 4e-08, 'epoch': 0.0}
{'eval_loss': 0.9270566701889038, 'eval_runtime': 175.3088, 'eval_samples_per_second': 11.1, 'eval_steps_per_second': 1.392, 'epoch': 0.0}
{'loss': 1.5352, 'grad_norm': 20.5, 'learning_rate': 6e-08, 'epoch': 0.0}
{'eval_loss': 0.9274019002914429, 'eval_runtime': 175.2915, 'eval_samples_per_second': 11.102, 'eval_steps_per_second': 1.392, 'epoch': 0.0}
{'loss': 2.1331, 'grad_norm': 29.5, 'learning_rate': 8e-08, 'epoch': 0.0}
{'eval_loss': 0.9270728826522827, 'eval_runtime': 175.313, 'eval_samples_per_second': 11.1, 'eval_steps_per_second': 1.392, 'epoch': 0.0}
{'loss': 1.3298, 'grad_norm': 21.625, 'learning_rate': 1e-07, 'epoch': 0.0}
{'eval_loss': 0.927306056022644, 'eval_runtime': 175.2426, 'eval_samples_per_second': 11.105, 'eval_steps_per_second': 1.392, 'epoch': 0.0}
{'loss': 1.7105, 'grad_norm': 18.75, 'learning_rate': 1.2e-07, 'epoch': 0.0}
{'eval_loss': 0.9273207187652588, 'eval_runtime': 175.2931, 'eval_samples_per_second': 11.101, 'eval_steps_per_second': 1.392, 'epoch': 0.0}
{'loss': 1.9137, 'grad_norm': 25.25, 'learning_rate': 1.4e-07, 'epoch': 0.0}
{'eval_loss': 0.9272845983505249, 'eval_runtime': 175.2056, 'eval_samples_per_second': 11.107, 'eval_steps_per_second': 1.393, 'epoch': 0.0}
{'loss': 1.2968, 'grad_norm': 17.75, 'learning_rate': 1.6e-07, 'epoch': 0.0}
{'eval_loss': 0.9271093010902405, 'eval_runtime': 175.586, 'eval_samples_per_second': 11.083, 'eval_steps_per_second': 1.39, 'epoch': 0.0}
{'loss': 1.307, 'grad_norm': 22.5, 'learning_rate': 1.8e-07, 'epoch': 0.0}
{'eval_loss': 0.9271284341812134, 'eval_runtime': 175.5423, 'eval_samples_per_second': 11.086, 'eval_steps_per_second': 1.39, 'epoch': 0.0}
{'train_runtime': 1977.9649, 'train_samples_per_second': 0.162, 'train_steps_per_second': 0.005, 'train_loss': 1.7217814803123475, 'epoch': 0.0}

-------------------------
Student evaluation for 0: 0.9812539219856262
Ensemble evaluation for 0: 0.9812539219856262
Teacher evaluation for 0: 1.035304307937622
-------------------------
==================================================
Ending Round 0 at: 2025-05-31 19:49:12
Completed in: 33m 48s
Total training time: 34m 0s
==================================================


==================================================
Starting Round 1 at: 2025-05-31 19:49:12
==================================================
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 0 that is less than the current step 113. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Round '1' model stored in: /projects/distilling_llms/model_log/2025-05-31/run_1/round_1
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|                                                                                                                                                                                                                                           | 0/10 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/h/klambert/slm_ensembles/train.py", line 313, in <module>
    main()
  File "/h/klambert/slm_ensembles/train.py", line 250, in main
    trainer.train()
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/trainer.py", line 2472, in _inner_training_loop
    self._evaluate(trial, ignore_keys_for_eval, skip_scheduler=True)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/trainer.py", line 3045, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/trainer.py", line 4154, in evaluate
    output = eval_loop(
  File "/h/klambert/slm_ensembles/train.py", line 169, in evaluation_loop
    output = super().evaluation_loop(dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/transformers/trainer.py", line 4348, in evaluation_loop
    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/h/klambert/slm_ensembles/train.py", line 161, in prediction_step
    kl_loss = self.compute_kl_loss(student_logits, ensemble_logits, teacher_logits, labels != -100)
  File "/h/klambert/slm_ensembles/train.py", line 130, in compute_kl_loss
    kl_loss = F.kl_div(student_probs, teacher_probs, log_target=True, reduction="none").sum(-1)
  File "/scratch/ssd004/scratch/klambert/slm_ensembles/venv/lib/python3.10/site-packages/torch/nn/functional.py", line 3396, in kl_div
    reduced = torch.kl_div(input, target, reduction_enum, log_target=log_target)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.64 GiB. GPU 0 has a total capacity of 44.56 GiB of which 4.60 GiB is free. Including non-PyTorch memory, this process has 39.95 GiB memory in use. Of the allocated memory 30.24 GiB is allocated by PyTorch, and 8.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
