{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Size Comparison\n",
    "Compare regular dataset vs clean dataset (without chat_text column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klambert/projects/aip-craffel/klambert/SLMensembles/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular dataset path: /scratch/klambert/dataset/tulu-3-sft-mixture-pretokenized\n",
      "Clean dataset path: /scratch/klambert/dataset/tulu-3-sft-mixture-pretokenized_clean\n",
      "Regular exists: True\n",
      "Clean exists: True\n"
     ]
    }
   ],
   "source": [
    "# Dataset paths\n",
    "regular_path = \"/scratch/klambert/dataset/tulu-3-sft-mixture-pretokenized\"\n",
    "clean_path = \"/scratch/klambert/dataset/tulu-3-sft-mixture-pretokenized_clean\"\n",
    "\n",
    "print(f\"Regular dataset path: {regular_path}\")\n",
    "print(f\"Clean dataset path: {clean_path}\")\n",
    "print(f\"Regular exists: {os.path.exists(regular_path)}\")\n",
    "print(f\"Clean exists: {os.path.exists(clean_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DISK USAGE COMPARISON ===\n",
      "Regular dataset: 2.90 GB (3,114,887,811 bytes)\n",
      "Clean dataset:   2.42 GB (2,603,123,108 bytes)\n",
      "Space saved:     488.06 MB (511,764,703 bytes)\n",
      "Reduction:       16.4%\n"
     ]
    }
   ],
   "source": [
    "# Get disk usage for both datasets\n",
    "def get_directory_size(path):\n",
    "    \"\"\"Get directory size in bytes using du command\"\"\"\n",
    "    result = subprocess.run(['du', '-sb', path], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        return int(result.stdout.split()[0])\n",
    "    return None\n",
    "\n",
    "def format_bytes(bytes_size):\n",
    "    \"\"\"Convert bytes to human readable format\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if bytes_size < 1024.0:\n",
    "            return f\"{bytes_size:.2f} {unit}\"\n",
    "        bytes_size /= 1024.0\n",
    "    return f\"{bytes_size:.2f} PB\"\n",
    "\n",
    "regular_size = get_directory_size(regular_path)\n",
    "clean_size = get_directory_size(clean_path)\n",
    "\n",
    "print(\"=== DISK USAGE COMPARISON ===\")\n",
    "print(f\"Regular dataset: {format_bytes(regular_size)} ({regular_size:,} bytes)\")\n",
    "print(f\"Clean dataset:   {format_bytes(clean_size)} ({clean_size:,} bytes)\")\n",
    "print(f\"Space saved:     {format_bytes(regular_size - clean_size)} ({regular_size - clean_size:,} bytes)\")\n",
    "print(f\"Reduction:       {((regular_size - clean_size) / regular_size * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "=== DATASET COMPARISON ===\n",
      "Regular dataset splits: ['train', 'test']\n",
      "Clean dataset splits: ['train', 'test']\n",
      "\n",
      "--- TRAIN SPLIT ---\n",
      "Regular - Samples: 192,704, Features: ['id', 'chat_text', 'input_ids', 'attention_mask', 'labels']\n",
      "Clean   - Samples: 192,704, Features: ['id', 'input_ids', 'attention_mask', 'labels']\n",
      "\n",
      "--- TEST SPLIT ---\n",
      "Regular - Samples: 1,943, Features: ['id', 'chat_text', 'input_ids', 'attention_mask', 'labels']\n",
      "Clean   - Samples: 1,943, Features: ['id', 'input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# Load and compare dataset metadata\n",
    "print(\"Loading datasets...\")\n",
    "regular_dataset = datasets.load_from_disk(regular_path)\n",
    "clean_dataset = datasets.load_from_disk(clean_path)\n",
    "\n",
    "print(\"\\n=== DATASET COMPARISON ===\")\n",
    "print(f\"Regular dataset splits: {list(regular_dataset.keys())}\")\n",
    "print(f\"Clean dataset splits: {list(clean_dataset.keys())}\")\n",
    "\n",
    "for split in regular_dataset.keys():\n",
    "    print(f\"\\n--- {split.upper()} SPLIT ---\")\n",
    "    print(f\"Regular - Samples: {len(regular_dataset[split]):,}, Features: {list(regular_dataset[split].features.keys())}\")\n",
    "    print(f\"Clean   - Samples: {len(clean_dataset[split]):,}, Features: {list(clean_dataset[split].features.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXAMPLE OF REMOVED CONTENT ===\n",
      "\n",
      "Regular dataset sample (first 300 chars of chat_text):\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'chat_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRegular dataset sample (first 300 chars of chat_text):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchat_text\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m regular_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[0;32m----> 5\u001b[0m     sample_text \u001b[38;5;241m=\u001b[39m \u001b[43mregular_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchat_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_text[:\u001b[38;5;241m300\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFull chat_text length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m characters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'chat_text'"
     ]
    }
   ],
   "source": [
    "# Show example of what was removed\n",
    "print(\"=== EXAMPLE OF REMOVED CONTENT ===\")\n",
    "print(\"\\nRegular dataset sample (first 300 chars of chat_text):\")\n",
    "if 'chat_text' in regular_dataset['train'].features:\n",
    "    sample_text = regular_dataset['train'][0]['chat_text']\n",
    "    print(f\"'{sample_text[:300]}...'\")\n",
    "    print(f\"\\nFull chat_text length: {len(sample_text)} characters\")\n",
    "else:\n",
    "    print(\"chat_text not found in regular dataset\")\n",
    "\n",
    "print(\"\\nClean dataset features:\")\n",
    "print(list(clean_dataset['train'].features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MEMORY USAGE COMPARISON ===\n",
      "Regular dataset memory: 112.00 B\n",
      "Clean dataset memory:   112.00 B\n",
      "Memory saved:           0.00 B\n"
     ]
    }
   ],
   "source": [
    "# Memory usage comparison when loading\n",
    "import sys\n",
    "\n",
    "def get_dataset_memory_usage(dataset):\n",
    "    \"\"\"Estimate memory usage of dataset\"\"\"\n",
    "    total_size = 0\n",
    "    for split_name, split_data in dataset.items():\n",
    "        # This is an approximation\n",
    "        total_size += sys.getsizeof(split_data)\n",
    "    return total_size\n",
    "\n",
    "print(\"=== MEMORY USAGE COMPARISON ===\")\n",
    "regular_memory = get_dataset_memory_usage(regular_dataset)\n",
    "clean_memory = get_dataset_memory_usage(clean_dataset)\n",
    "\n",
    "print(f\"Regular dataset memory: {format_bytes(regular_memory)}\")\n",
    "print(f\"Clean dataset memory:   {format_bytes(clean_memory)}\")\n",
    "print(f\"Memory saved:           {format_bytes(regular_memory - clean_memory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SUMMARY TABLE ===\n",
      "   Dataset Disk Size Train Samples Test Samples  Features\n",
      "   Regular   2.90 GB       192,704        1,943         5\n",
      "     Clean   2.42 GB       192,704        1,943         4\n",
      "Difference 488.06 MB             0            0         1\n"
     ]
    }
   ],
   "source": [
    "# Summary table\n",
    "summary_data = {\n",
    "    'Dataset': ['Regular', 'Clean', 'Difference'],\n",
    "    'Disk Size': [format_bytes(regular_size), format_bytes(clean_size), format_bytes(regular_size - clean_size)],\n",
    "    'Train Samples': [f\"{len(regular_dataset['train']):,}\", f\"{len(clean_dataset['train']):,}\", \"0\"],\n",
    "    'Test Samples': [f\"{len(regular_dataset['test']):,}\", f\"{len(clean_dataset['test']):,}\", \"0\"],\n",
    "    'Features': [len(regular_dataset['train'].features), len(clean_dataset['train'].features), \n",
    "                len(regular_dataset['train'].features) - len(clean_dataset['train'].features)]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"=== SUMMARY TABLE ===\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns were removed\n",
    "regular_features = set(regular_dataset['train'].features.keys())\n",
    "clean_features = set(clean_dataset['train'].features.keys())\n",
    "\n",
    "removed_features = regular_features - clean_features\n",
    "print(f\"\\n=== REMOVED FEATURES ===\")\n",
    "print(f\"Removed columns: {list(removed_features)}\")\n",
    "print(f\"Remaining columns: {list(clean_features)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
