
# TODO before each run:
# change hyperparameter
# change custom_run_name
# change description
# change job name in train.sh
# clean up and safely store csv files
# remove empty directories in past_run_dirs
# uncomment and comment out past_run_dirs
# launch the sbatch script with custom names
 
TODO immediate: 
[x] find the best learning rate (convergence)        [5e-5]
[] compare alpha values (distillation)               [0, 0.5, 0.8, 1]
[] steps per round (more steps = more overfit)       [1000, 1200, 2000, 5000]
[] batch size (number of smaples per optimization)   [2, 4, 8]
[] KL temperature (later for distillation)           [0.5, 0.7, 1.0, 1.5, 2.0]
[] fix the eval logging in SLM_ensembles

TODO next:
[] identify whether sequential training works at all:
    [] find the best hyperparameters for a single model which brings the loss down
    [] add a second model and see what we can do to make it better

[] run pure SFT
[] dataset curation - generate predictions with the teacher (if distillation works)
[] C-BTM - train specialized ensemble members, but with instruction tunning data

TODO near future:
[] Explore other methods to make the ensemble better
[] Feedback-based distillation
[] Benchmark GPU usage and inference costs
[] Paper

