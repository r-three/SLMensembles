
# TODO before each run:
# change hyperparameter
# change custom_run_name
# change description
# change job name in train.sh
# clean up and safely store csv files
# remove empty directories in past_run_dirs
# uncomment and comment out past_run_dirs
# launch the sbatch script with custom names
 
TODO immediate: 
[x] find the best learning rate (convergence)                           [5e-5]
[] Set up model checkpointing and re-run the scripts
[] compare alpha values (distillation) and see that the loss plateaus   [0, 0.5, 0.8, 1]
[] steps per round (more steps = more overfit)                          [1000, 1200, 2000, 5000]
[] batch size (number of smaples per optimization)                      [2, 4, 8]
[] KL temperature (later for distillation)                              [0.5, 0.7, 1.0, 1.5, 2.0]
[x] set up better eval metrics for the ensembles
[x] finds ways to concatenate csv file metrics

TODO next:
[] identify whether sequential training works at all:
    [] find the best hyperparameters for a single model which brings the loss down
    [] add a second model and see what we can do to make it better
[] run pure SFT
[] dataset curation - generate predictions with the teacher (if distillation works)
[] C-BTM - train specialized ensemble members, but with instruction tunning data

TODO near future:
[] Explore other methods to make the ensemble better
[] Feedback-based distillation
[] Benchmark GPU usage and inference costs
[] Turn this into a distillation framework that can be leveraged
[] Paper

Meeting Questions:
[] is it worth it setting up GPU profiling?
