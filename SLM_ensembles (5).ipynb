{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOGIsvlRY1sb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import scipy.signal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "D8R8MWmsqwlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(uploaded)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YQZWdy61gBzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert strings to floats\n",
        "# for col in [\"round\", \"step\", \"eval_loss\", \"train_loss\", \"kl_loss\", \"perplexity\", \"ensemble_size\"]:\n",
        "#     df[col] = pd.to_numeric(df[col], errors=\"coerce\")"
      ],
      "metadata": {
        "id": "uKReB4fTh86D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student next token train loss over rounds\n",
        "student_train_df = df[(df['role'] == 'student') & (df['phase'] == 'train') & (df['function'] == 'compute_loss')]\n",
        "student_train_df = student_train_df.sort_values(by=[\"round\", \"step\"]).reset_index(drop=True)\n",
        "student_train_df[\"global_step\"] = range(len(student_train_df))\n",
        "\n",
        "round_changes = student_train_df['round'].diff().fillna(0) != 0\n",
        "for idx in student_train_df[round_changes].index:\n",
        "    plt.axvline(x=student_train_df.loc[idx, 'global_step'], color='gray', linestyle='--', alpha=0.3)\n",
        "\n",
        "plt.figure(figsize=(50, 6))\n",
        "plt.plot(student_train_df[\"global_step\"], scipy.signal.medfilt(student_train_df[\"train_next_token_loss\"], 11), label=\"Next Token Loss\", linestyle='-')\n",
        "plt.title(\"Training Next Token Prediction Loss Across Rounds\")\n",
        "plt.xlabel(\"Global Step\")\n",
        "plt.ylabel(\"Train Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tcHicnXTh65L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student train loss (kl) over rounds\n",
        "plt.figure(figsize=(50, 6))\n",
        "plt.plot(student_train_df[\"global_step\"], scipy.signal.medfilt(student_train_df[\"train_kl_loss\"], 11), label=\"KL Loss\", linestyle='-')\n",
        "plt.title(\"Training KL Loss Across Rounds\")\n",
        "plt.xlabel(\"Global Step\")\n",
        "plt.ylabel(\"Train Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uLxLAtgjflJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student train loss (hybrid) over rounds\n",
        "plt.figure(figsize=(50, 6))\n",
        "plt.plot(student_train_df[\"global_step\"], scipy.signal.medfilt(student_train_df[\"train_loss\"], 11), label=\"Next Token Loss\", linestyle='-')\n",
        "plt.title(\"Training Loss Across Rounds\")\n",
        "plt.xlabel(\"Global Step\")\n",
        "plt.ylabel(\"Train Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4QeT2LdQfoxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per round logging of training loss ?"
      ],
      "metadata": {
        "id": "TJet_BGTft89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per round logging of student kl eval loss\n",
        "student_eval_df = df[\n",
        "    (df[\"phase\"] == \"eval\") &\n",
        "    (df[\"role\"] == \"student\") &\n",
        "    (df[\"function\"] == \"prediction_step\")\n",
        "]\n",
        "\n",
        "student_eval_df = student_eval_df.sort_values(by=[\"round\", \"step\"]).reset_index(drop=True)\n",
        "student_eval_df[\"global_step\"] = range(len(student_eval_df))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(student_eval_df[\"global_step\"], scipy.signal.medfilt(student_eval_df[\"eval_kl_loss\"], 11), label=\"Student KL Eval Loss\", linestyle='-')\n",
        "plt.title(\"Student KL Eval Loss\")\n",
        "plt.xlabel(\"Global Step\")\n",
        "plt.ylabel(\"Eval KL Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mxm4mrVHf1lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per round logging of student LM eval loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(student_eval_df[\"global_step\"], scipy.signal.medfilt(student_eval_df[\"eval_loss\"], 11), label=\"Student LM Eval Loss\", linestyle='-')\n",
        "plt.title(\"Student LM Eval Loss\")\n",
        "plt.xlabel(\"Global Step\")\n",
        "plt.ylabel(\"Train Loss\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"Eval LM Loss\")\n",
        "plt.title(\"Student LM Eval Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QK5UYRcJfyJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student vs Teacher vs Ensemble performance over rounds on next_token_prediction, custom evaluation method\n",
        "df_eval = df[df.phase==\"custom_eval\"]\n",
        "df_teacher = df_eval[df_eval.role==\"teacher\"].sort_values(\"round\")\n",
        "df_ensemble = df_eval[df_eval.role==\"ensemble\"].sort_values(\"round\")\n",
        "df_student = df_eval[df_eval.role==\"student\"].sort_values(\"round\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(\n",
        "    df_teacher[\"round\"],\n",
        "    df_teacher[\"eval_loss\"],\n",
        "    marker=\"o\", linestyle=\"-\", color=\"C1\", label=\"teacher\"\n",
        ")\n",
        "plt.plot(\n",
        "    df_ensemble[\"round\"],\n",
        "    df_ensemble[\"eval_loss\"],\n",
        "    marker=\"o\", linestyle=\":\", color=\"C2\", label=\"ensemble\"\n",
        ")\n",
        "plt.plot(\n",
        "    df_student[\"round\"],\n",
        "    df_student[\"eval_loss\"],\n",
        "    marker=\"o\", linestyle=\"-\", color=\"C3\", label=\"student\"\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"Eval Loss\")\n",
        "plt.title(\"Next Token Prediction loss across rounds\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AQgP65U_hNG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student vs Teacher vs Ensemble perplexity over rounds, custom evaluation method\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(\n",
        "    df_teacher[\"round\"],\n",
        "    df_teacher[\"perplexity\"],\n",
        "    marker=\"o\", linestyle=\"-\", color=\"C1\", label=\"teacher\"\n",
        ")\n",
        "plt.plot(\n",
        "    df_ensemble[\"round\"],\n",
        "    df_ensemble[\"perplexity\"],\n",
        "    marker=\"o\", linestyle=\":\", color=\"C2\", label=\"ensemble\"\n",
        ")\n",
        "plt.plot(\n",
        "    df_student[\"round\"],\n",
        "    df_student[\"perplexity\"],\n",
        "    marker=\"o\", linestyle=\"-\", color=\"C3\", label=\"student\"\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.title(\"Perplexity across rounds\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tqUkK9KV34AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval loss vs ensemble size\n",
        "plt.figure(figsize=(10, 6))\n",
        "df_ensemble = df[(df[\"role\"] == \"ensemble\") & (df[\"phase\"] == \"custom_eval\")]\n",
        "df_teacher = df[(df[\"role\"] == \"teacher\") & (df[\"phase\"] == \"custom_eval\")]\n",
        "\n",
        "plt.plot(df_ensemble[\"ensemble_size\"], df_ensemble[\"eval_loss\"], label=\"Ensemble\", marker=\"o\")\n",
        "plt.plot(df_teacher[\"ensemble_size\"], df_teacher[\"eval_loss\"], label=\"Teacher\", marker=\"o\")\n",
        "plt.xlabel(\"Ensemble Size\")\n",
        "plt.ylabel(\"Eval Loss\")\n",
        "plt.title(\"Eval Loss vs Ensemble Size\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JlU85e47lzoM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}