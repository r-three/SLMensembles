import os
import torch
from datetime import datetime
import glob

# Model and dataset setup
seed = 42
teacher_model_name = "Qwen/Qwen2.5-7B-Instruct"
student_model_name = "Qwen/Qwen2.5-0.5B-Instruct"
student_vocab_size = 151936 # 152064
tokenizer_name = "Qwen/Qwen2.5-0.5B-Instruct"

teacher_device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
student_device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")

teacher_eval = (0.7968094515065487, 2.218451499938965)

# Output paths
base_output_dir = "/your_directory/slm_ensembles/model_log"
log_dir = "/your_directory/slm_ensembles/csv_logs"
logit_cache_path = "/your_directory/slm_ensembles/teacher_logits"
dataset_path = "/your_directory/slm_ensembles/tulu-3-sft-mixture-pretokenized"
synthetic_dataset_path = "/your_directory/slm_ensembles/synthetic_dataset"

# Data
dataset_name = "allenai/tulu-3-sft-mixture"
dataset_type = "full"  # "single" or "batch" or "full"
synthetic_data = False

# Run parameters - to change during every run
ddp = False
steps_per_round = -1
num_train_epochs = 6
total_rounds = 1  # number of ensemble models
id_string = "Experiment with hyperparameters to check if distillation works"
description = "Alpha tweaking: alpha = 0.8"
custom_run_name = "alpha08_hyperparameters"
checkpoint_path = None  # model
checkpoint_log_path = None  # csv
overwrite_csv = False  # overwrite csv file, if it exists, with the new run

# Ensembles
ensemble_model_names = []
ensemble_members = []  # Full path of ensemble models which we want to load

# Hyperparameters
learning_rate = 5e-5
kl_temperature = 1.0
alpha = 0.8

# Training args
eval_steps = 40
logging_steps = 40
save_steps = 500
save_total_limit = 2
per_device_train_batch_size = 4
eval_batch_size = 4
gradient_accumulation_steps = 8

# Logging columns
CSV_COLUMNS = [
    "function",
    "timestamp",
    "overall_elapsed",
    "round_duration",
    "round_num",
    "phase",
    "role",
    "step",
    "train_loss",
    "train_kl_loss",
    "train_next_token_loss",
    "eval_loss",
    "eval_kl_loss",
    "grad_norm",
    "learning_rate",
    "alpha",
    "tags",
    "metadata",
]


def get_directory(output_dir):
    current_date = datetime.now().strftime("%d-%m-%Y")
    date_dir = os.path.join(output_dir, current_date)

    if output_dir == log_dir:
        return date_dir

    existing_runs = []
    if os.path.exists(date_dir):
        run_dirs = glob.glob(os.path.join(date_dir, "run_*"))

        for dir_path in run_dirs:
            try:
                run_num = int(os.path.basename(dir_path).split("_")[1])
                existing_runs.append(run_num)
            except (ValueError, IndexError):
                continue

    next_run = 1
    if existing_runs:
        next_run = max(existing_runs) + 1

    if custom_run_name is not None:
        run_dir = os.path.join(date_dir, f"run_{next_run}_{custom_run_name}")
    else:
        run_dir = os.path.join(date_dir, f"run_{next_run}")

    os.makedirs(run_dir, exist_ok=True)
    return run_dir


def get_training_args(output_dir):
    from trl import SFTConfig

    return SFTConfig(
        output_dir=output_dir,
        overwrite_output_dir=False,
        report_to="none",
        hub_model_id=None,
        learning_rate=learning_rate,
        lr_scheduler_type="constant",
        warmup_steps=50,
        per_device_train_batch_size=per_device_train_batch_size,
        per_device_eval_batch_size=eval_batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        gradient_checkpointing=False,
        bf16=True,
        remove_unused_columns=False,
        max_steps=steps_per_round,
        num_train_epochs=num_train_epochs,
        eval_strategy="steps",
        eval_steps=eval_steps,
        eval_on_start=False,
        logging_strategy="steps",
        logging_steps=logging_steps,
        save_strategy="no",
    )

