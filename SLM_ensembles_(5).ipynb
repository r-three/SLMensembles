{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOGIsvlRY1sb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import scipy.signal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "D8R8MWmsqwlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = {}\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    name = filename.split(\".\")[0]  # use filename as ID\n",
        "    df = pd.read_csv(filename)\n",
        "    df[\"source\"] = name  # tag the source\n",
        "    dfs[name] = df"
      ],
      "metadata": {
        "id": "foA41CWynj5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat(dfs.values(), ignore_index=True) # for  one big group-level analysis"
      ],
      "metadata": {
        "id": "dMVTWvYOns6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (kl loss) 0 <= alpha <= 1 (next_token loss)"
      ],
      "metadata": {
        "id": "i8Rg8kdY3y8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student train loss (hybrid) over rounds\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for name, df in dfs.items():\n",
        "    student_train_df = df[\n",
        "        (df[\"role\"] == \"student\") &\n",
        "        (df[\"phase\"] == \"train\") &\n",
        "        (df[\"function\"] == \"compute_loss\")\n",
        "    ].sort_values(by=[\"round\", \"step\"]).reset_index(drop=True)\n",
        "\n",
        "    student_train_df[\"global_step\"] = range(len(student_train_df))\n",
        "    smoothed_loss = scipy.signal.medfilt(student_train_df[\"train_loss\"], 11)\n",
        "\n",
        "    plt.plot(student_train_df[\"global_step\"], smoothed_loss, label=f\"{name}\")\n",
        "\n",
        "plt.title(\"Student Hybrid Training Loss Across Rounds\")\n",
        "plt.xlabel(\"Global Step\")\n",
        "plt.ylabel(\"Train Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ql9f0wuMnzfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student train loss (kl) over rounds\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for name, df in dfs.items():\n",
        "    student_train_df = df[\n",
        "        (df[\"role\"] == \"student\") &\n",
        "        (df[\"phase\"] == \"train\") &\n",
        "        (df[\"function\"] == \"compute_loss\")\n",
        "    ].sort_values(by=[\"round\", \"step\"]).reset_index(drop=True)\n",
        "\n",
        "    student_train_df[\"global_step\"] = range(len(student_train_df))\n",
        "    smoothed_loss = scipy.signal.medfilt(student_train_df[\"train_kl_loss\"], 11)\n",
        "\n",
        "    plt.plot(student_train_df[\"global_step\"], smoothed_loss, label=f\"{name}\")\n",
        "\n",
        "plt.title(\"Student Training KL Loss Across Rounds\")\n",
        "plt.xlabel(\"Global Step\")\n",
        "plt.ylabel(\"Train Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uLxLAtgjflJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student train loss (next token) over rounds\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for name, df in dfs.items():\n",
        "    student_train_df = df[\n",
        "        (df[\"role\"] == \"student\") &\n",
        "        (df[\"phase\"] == \"train\") &\n",
        "        (df[\"function\"] == \"compute_loss\")\n",
        "    ].sort_values(by=[\"round\", \"step\"]).reset_index(drop=True)\n",
        "\n",
        "    student_train_df[\"global_step\"] = range(len(student_train_df))\n",
        "    smoothed_loss = scipy.signal.medfilt(student_train_df[\"train_next_token_loss\"], 11)\n",
        "\n",
        "    plt.plot(student_train_df[\"global_step\"], smoothed_loss, label=f\"{name}\")\n",
        "\n",
        "plt.title(\"Student Next Token 'Training' Loss Across Rounds\")\n",
        "plt.xlabel(\"Global Step\")\n",
        "plt.ylabel(\"Train Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4QeT2LdQfoxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per round logging of training loss ?"
      ],
      "metadata": {
        "id": "TJet_BGTft89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per round logging of student LM eval loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for name, df in dfs.items():\n",
        "    student_eval_df = df[\n",
        "        (df[\"role\"] == \"student\") &\n",
        "        (df[\"phase\"] == \"eval\") &\n",
        "        (df[\"function\"] == \"prediction_step\")\n",
        "    ].sort_values(by=[\"round\", \"step\"]).reset_index(drop=True)\n",
        "\n",
        "    student_eval_df[\"global_step\"] = range(len(student_eval_df))\n",
        "    smoothed_loss = scipy.signal.medfilt(student_eval_df[\"eval_loss\"], 11)\n",
        "\n",
        "    plt.plot(student_eval_df[\"global_step\"], smoothed_loss, label=f\"{name}\")\n",
        "\n",
        "plt.title(\"Student LM Eval Loss\")\n",
        "plt.xlabel(\"Global Step\")\n",
        "plt.ylabel(\"Eval LM Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QK5UYRcJfyJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Per round logging of student kl eval loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for name, df in dfs.items():\n",
        "    student_eval_df = df[\n",
        "        (df[\"role\"] == \"student\") &\n",
        "        (df[\"phase\"] == \"eval\") &\n",
        "        (df[\"function\"] == \"prediction_step\")\n",
        "    ].sort_values(by=[\"round\", \"step\"]).reset_index(drop=True)\n",
        "\n",
        "    student_eval_df[\"global_step\"] = range(len(student_eval_df))\n",
        "    smoothed_loss = scipy.signal.medfilt(student_eval_df[\"eval_kl_loss\"], 11)\n",
        "\n",
        "    plt.plot(student_eval_df[\"global_step\"], smoothed_loss, label=f\"{name}\")\n",
        "\n",
        "plt.title(\"Student KL Eval Loss\")\n",
        "plt.xlabel(\"Global Step\")\n",
        "plt.ylabel(\"Eval KL Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mxm4mrVHf1lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student vs Teacher vs Ensemble performance over rounds on next_token_prediction, custom evaluation method\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "first_run_id = list(dfs.keys())[0]\n",
        "df_teacher = dfs[first_run_id]\n",
        "df_teacher = df_teacher[\n",
        "    (df_teacher[\"phase\"] == \"custom_eval\") &\n",
        "    (df_teacher[\"role\"] == \"teacher\")\n",
        "].sort_values(\"round\")\n",
        "\n",
        "plt.plot(\n",
        "    df_teacher[\"round\"],\n",
        "    df_teacher[\"eval_loss\"],\n",
        "    linestyle=\"-\",\n",
        "    marker=\"x\",\n",
        "    color=\"black\",\n",
        "    label=\"Teacher\"\n",
        ")\n",
        "\n",
        "for name, df in dfs.items():\n",
        "    df_eval = df[df[\"phase\"] == \"custom_eval\"]\n",
        "\n",
        "    # --- STUDENT ---\n",
        "    df = df[~df[\"tags\"].fillna(\"\").str.contains(\"initial eval\")]\n",
        "    df_student = df_eval[df_eval[\"role\"] == \"student\"]\n",
        "\n",
        "    df_student = df_student.sort_values(\"timestamp\")\n",
        "    df_student = df_student.groupby(\"round\", as_index=False).last()\n",
        "\n",
        "    plt.plot(\n",
        "        df_student[\"round\"],\n",
        "        df_student[\"eval_loss\"],\n",
        "        linestyle=\"-\",\n",
        "        marker=\"o\",\n",
        "        label=f\"Student ({name})\"\n",
        "    )\n",
        "\n",
        "    # --- ENSEMBLE ---\n",
        "    df_ensemble = df_eval[df_eval[\"role\"] == \"ensemble\"]\n",
        "    df_ensemble = df_ensemble.sort_values(\"timestamp\")\n",
        "    # df_ensemble = df_ensemble.groupby(\"round\", as_index=False).last()\n",
        "\n",
        "    plt.plot(\n",
        "        df_ensemble[\"round\"],\n",
        "        df_ensemble[\"eval_loss\"],\n",
        "        linestyle=\":\",\n",
        "        marker=\"s\",\n",
        "        label=f\"Ensemble ({name})\"\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"Eval Loss\")\n",
        "plt.title(\"Next Token Prediction loss across rounds\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "8Hor3iW06GwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Student vs Teacher vs Ensemble perplexity over rounds, custom evaluation method\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "first_run_id = list(dfs.keys())[0]\n",
        "df_teacher = dfs[first_run_id]\n",
        "df_teacher = df_teacher[\n",
        "    (df_teacher[\"phase\"] == \"custom_eval\") &\n",
        "    (df_teacher[\"role\"] == \"teacher\")\n",
        "].sort_values(\"round\")\n",
        "\n",
        "plt.plot(\n",
        "    df_teacher[\"round\"],\n",
        "    df_teacher[\"perplexity\"],\n",
        "    linestyle=\"-\",\n",
        "    marker=\"x\",\n",
        "    color=\"black\",\n",
        "    label=\"Teacher\"\n",
        ")\n",
        "\n",
        "for name, df in dfs.items():\n",
        "    df_eval = df[df[\"phase\"] == \"custom_eval\"]\n",
        "\n",
        "    # --- STUDENT ---\n",
        "    df = df[~df[\"tags\"].fillna(\"\").str.contains(\"initial eval\")]\n",
        "    df_student = df_eval[df_eval[\"role\"] == \"student\"]\n",
        "\n",
        "    df_student = df_student.sort_values(\"timestamp\")\n",
        "    df_student = df_student.groupby(\"round\", as_index=False).last()\n",
        "\n",
        "    plt.plot(\n",
        "        df_student[\"round\"],\n",
        "        df_student[\"perplexity\"],\n",
        "        linestyle=\"-\",\n",
        "        marker=\"o\",\n",
        "        label=f\"Student ({name})\"\n",
        "    )\n",
        "\n",
        "    # --- ENSEMBLE ---\n",
        "    df_ensemble = df_eval[df_eval[\"role\"] == \"ensemble\"]\n",
        "    df_ensemble = df_ensemble.sort_values(\"timestamp\")\n",
        "    df_ensemble = df_ensemble.groupby(\"round\", as_index=False).last()\n",
        "\n",
        "    plt.plot(\n",
        "        df_ensemble[\"round\"],\n",
        "        df_ensemble[\"perplexity\"],\n",
        "        linestyle=\":\",\n",
        "        marker=\"s\",\n",
        "        label=f\"Ensemble ({name})\"\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.title(\"Perplexity across rounds\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tqUkK9KV34AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval loss vs ensemble size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot teacher once (constant)\n",
        "first_run_id = list(dfs.keys())[0]\n",
        "df_teacher = dfs[first_run_id]\n",
        "df_teacher = df_teacher[\n",
        "    (df_teacher[\"role\"] == \"teacher\") &\n",
        "    (df_teacher[\"phase\"] == \"custom_eval\")\n",
        "]\n",
        "\n",
        "plt.plot(\n",
        "    df_teacher[\"ensemble_num\"],\n",
        "    df_teacher[\"eval_loss\"],\n",
        "    label=\"Teacher\",\n",
        "    marker=\"x\",\n",
        "    color=\"black\"\n",
        ")\n",
        "\n",
        "# Plot ensemble eval_loss vs ensemble_size per file\n",
        "for name, df in dfs.items():\n",
        "    df_ensemble = df[\n",
        "        (df[\"role\"] == \"ensemble\") &\n",
        "        (df[\"phase\"] == \"custom_eval\")\n",
        "    ]\n",
        "\n",
        "    plt.plot(\n",
        "        df_ensemble[\"ensemble_num\"],  # or \"ensemble_size\"\n",
        "        df_ensemble[\"eval_loss\"],\n",
        "        marker=\"s\",\n",
        "        linestyle=\":\",\n",
        "        label=f\"Ensemble ({name})\"\n",
        "    )\n",
        "\n",
        "plt.xlabel(\"Ensemble Size\")\n",
        "plt.ylabel(\"Eval Loss\")\n",
        "plt.title(\"Eval Loss vs Ensemble Size\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JlU85e47lzoM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}