Core bet: Instead of distilling a big teacher into a single small student, distill into an ensemble of small students that can be run in parallel with minimal inter-device communication.

This is different from â€œjust ensembling small modelsâ€: youâ€™re proposing teacher-guided specialization and training schemes so the ensemble approximates a much larger modelâ€™s distribution, not merely averaging a bunch of off-the-shelf small LMs.

Your doc explicitly tees up the open questions your experiments should answer:
- How to train members (sequential/parallel/independent/joint)?
- Which distillation objective?
- Whether/when specialization helps.

Some key challenges and questions this project can address include:

1. Ensemble Training Strategies
   - How should ensemble members be trained?
   - Should they be trained sequentially or in parallel?
   - Should they be trained independently or jointly?
   - Should each member be trained on a different data distribution to encourage specialization?

2. Distillation Objectives
   - What is the best objective for distilling knowledge from the large model into the ensemble?
   - Should the distillation objective focus on distribution matching between the large LM and the ensemble?
   - Or maximizing likelihood on the text generated by the large LM?

3. Data Selection and Distribution
   - What type of data should we be using for distillation?

4. Training Implementation
   - The simplest implementation for an ensemble is to iterate over the ensemble members and perform an independent forward pass for each model.
   - Is there a more efficient way to perform an ensemble forward pass that avoids looping (e.g., packing the ensemble membersâ€™ weights)?

5. Scaling Laws for Ensemble Distillation
   - Once weâ€™ve trained ensembles of various sizes, can we estimate scaling laws that estimate the performance of an ensemble as a function of (1) the size of the ensemble members, (2) the number of ensemble members?

What â€œgoodâ€ looks like (publishable end-state):

1. Fair baselines
   - Teacher (frozen) and Teacher (finetuned on your distill data with the same budget).
   - Single student (distilled).
   - Ensemble of N students (various N).
   - Ensemble of N students (raw; untrained) to show the gain from distillation and ensembling.

2. Clear metrics and plots
   - Token-level: Perplexity / cross-entropy on held-out text.
   - Task-level: a small suite (e.g., 3â€“5 standard evalsâ€”ARC, HellaSwag, MMLU-lite, GSM8K subset). Report accuracy and variance across seeds.
   - Scaling curves: performance vs. (a) number of members and (b) size per member (this is one of your listed research questions).

3. A concrete win
   - ex. Quality at fixed compute: An ensemble of KÃ—0.5B matches or beats a single 1.5â€“3B student on perplexity and at least 2 downstream tasks.
   - ex. Latency/throughput on commodity hardware: Parallel ensemble on K devices achieves competitive latency and higher accuracy than a single small model.
   - ex. Specialization advantage: Ensembles trained with specialization (routing or clustered data) outperform same-size, non-specialized ensembles.
   - ex. Framework for distillation + routings.




Plan:

What I did:
- 

Next Steps:
- 

Fixes to add to main:
- transfer over dataset preprocessing script Fixes
- train and test loader Fixes in simple_utils.py
- ensemble_model_name -> ensemble_dirs in main (if config.ensemble_dirs is not None:)
- simple_ensemble.py - ensemble model loading code 
- 


Week 1: Method Exploration â€” What Works and What Doesnâ€™t

Goal: Identify which distillation + ensembling methods actually improve performance. Prioritize method-level comparisons over fine-tuning.

ðŸ”¹1.1 â€“ Setup for Apples-to-Apples Comparison
   [X] Fix dataset splits and logging
   [X] Generate new teacher logits with ids for clustering
   [O] Regenerate the dataset with the new chat templates and ids
   [X] Re-cache the teacher logits with the ids
   [X] Implement CSV logging for all experiments
   [] Run Teacher (frozen) and Teacher (finetuned) on distillation dataset
   [] Evaluate teacher on downstream tasks (ARC, HellaSwag, MMLU-lite, GSM8K)

ðŸ”¹1.2 â€“ Ensemble Formation Method Comparison
   Run 2â€“3 members per method to reduce cost but expose differences early.
   Track: perplexity, task accuracy, ECE, variance.
   Have the following models saved:

   A. Independent Deep Ensembles
      [] Different seeds / modest data permutations (Inference via probability averaging)

   B. Specialization (CBTM-style)
      [] Finalize dataset clustering loading
      [] Train students on cluster-specific subsets (80/20)
      Inference via: uniform averaging or routing w/ learned mixture weights (KL loss)

   C. Boosting / Residual Fitting
      [] First student = generalist
      [] Next student = trained on top-q% KL-divergent examples from round r (q âˆˆ {10%, 25%})
      Optional: weighted KD loss or residual loss

   D. Baseline Student
      [] Secure a single small trained model using KL or hybrid loss on full data for evaluation

   E. Raw Student (ensemble)
      [] Untrained to measure against the other methods

ðŸ”¹1.3 â€“ Extra methods

   A. Negative Correlation Learning (NCL)
      [] Train ensemble members to minimize both prediction error AND correlation between members
      [] Use Î»-penalty term: Loss = MSE + Î» * correlation_penalty
      [] Forces diversity while maintaining accuracy
      [] Inference: Simple averaging (no routing needed)

   B. Bagging with Random Data Subsets
      [] Each student trained on 60-80% random sample of full dataset (with replacement)
      [] Compare against clustering-based specialization (Method B)
      [] Less structured than CBTM but potentially more robust

   C. Synthetic dataset
      [] Evaluate training on synthetic dataset

ðŸ”¹1.4 â€“ Evaluate; Triage output:
   [] Evaluate models trained on different methods on the Olmes benchmark
      - What methods work best (on perplexity + accuracy + performance)?
      - Does specialization help (CBTM/routing)?
      - Do any methods fail (e.g. boosting doesnâ€™t help)?

Week 2 â€“ Targeted Optimization + Robustness

Goal: Tune the best method(s) from Week 1. Improve performance, analyze behavior, and test generalization.

ðŸ”¹2.1 Optimization
   [] Run grid: Î± âˆˆ {0, 0.5, 1}, Ï„ âˆˆ {1, 2}
   Sweep for:
      [] Single student
      [] Best ensemble method(s)
   Track: dev perplexity, task score, ECE

ðŸ”¹2.2 Generalization + Robustness
   [] Evaluate:
      - OOD domain/task
      - Confidence calibration (ECE)
      - Train routing model (if CBTM specialization selected)
        (Use teacher logits to train router on cluster index or gating weights)

ðŸ”¹2.3 Ablations
   - Does routing help vs uniform averaging?
   - Whatâ€™s the effect of member diversity (random init vs data cluster)?

Week 3 â€“ Scaling Laws + Final Story

Goal: Show why ensemble distillation matters at scale.

ðŸ”¹3.1 Scaling Experiments
   [] Fix compute budget (tokens/flops)
   Vary:
      [] Ensemble size K âˆˆ {1, 2, 4, 8, 12}
      [] (Optional) member size: 0.5B vs 1B
   Plot:
      [] Perplexity vs K
      [] Accuracy vs K
      [] Calibration vs K
      [] Model count vs performance
      [] Param count vs Performance (optional)
      [] Measure latency / report on GPU usage

ðŸ”¹3.2 Final Table + Win Conditions
   - Leaderboard: each method
   - Scaling laws
   - Graphs and plots
   - ECE/OOD table
   - Highlight win

Notes:
- Boosting helps when successive students see harder or different slices (reweighting by prior errors). With limited capacity, you wonâ€™t close all residuals, but you can still gain by concentrating each new student where the ensemble is weak.
- Temperature selection: choose Ï„ that minimizes dev perplexity when distilling; donâ€™t guess. Reuse that Ï„ for all comparable runs.
- Compute parity: when comparing single vs ensemble, keep total tokens/flops in the same ballparkâ€”or report both quality and cost.

Questions:
- Hard-example mining: top-q% selection (next round's dataset gets the highest scored samples) or weighted KD (weigh per-example loss)?
- Probability averaging for each ensemble member after training?
- Variable Î± throughout the training? (i.e. start out with pure distillation, finish with next token, or the other way around)?