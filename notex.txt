What I did:
- launched logit_caching with temperature = 1.0, and alpha = 0.6
- teacher logit caching code done
- terminated model training script for 'test_run' run - model checkpoints saved

Next:
- fix the checkpoint model loading to be able to load from a distributed checkpoint directory, just like in simple_eval
- test usage of cached logits in ensembling code
- launch next ensembling run using the boosting branch and cached ensemble logits
- check yizhen's progress








Core bet: Instead of distilling a big teacher into a single small student, distill into an ensemble of small students that can be run in parallel with minimal inter-device communication.

This is different from ‚Äújust ensembling small models‚Äù: you‚Äôre proposing teacher-guided specialization and training schemes so the ensemble approximates a much larger model‚Äôs distribution, not merely averaging a bunch of off-the-shelf small LMs.

Your doc explicitly tees up the open questions your experiments should answer:
- How to train members (sequential/parallel/independent/joint)?
- Which distillation objective?
- Whether/when specialization helps.

Some key challenges and questions this project can address include:

1. Ensemble Training Strategies
   - How should ensemble members be trained?
   - Should they be trained sequentially or in parallel?
   - Should they be trained independently or jointly?
   - Should each member be trained on a different data distribution to encourage specialization?

2. Distillation Objectives
   - What is the best objective for distilling knowledge from the large model into the ensemble?
   - Should the distillation objective focus on distribution matching between the large LM and the ensemble?
   - Or maximizing likelihood on the text generated by the large LM?

3. Data Selection and Distribution
   - What type of data should we be using for distillation?

4. Training Implementation
   - The simplest implementation for an ensemble is to iterate over the ensemble members and perform an independent forward pass for each model.
   - Is there a more efficient way to perform an ensemble forward pass that avoids looping (e.g., packing the ensemble members‚Äô weights)?

5. Scaling Laws for Ensemble Distillation
   - Once we‚Äôve trained ensembles of various sizes, can we estimate scaling laws that estimate the performance of an ensemble as a function of (1) the size of the ensemble members, (2) the number of ensemble members?

What ‚Äúgood‚Äù looks like (publishable end-state):

1. Fair baselines
   - Teacher (frozen) and Teacher (finetuned on your distill data with the same budget).
   - Single student (distilled).
   - Ensemble of N students (various N).
   - Ensemble of N students (raw; untrained) to show the gain from distillation and ensembling.

2. Clear metrics and plots
   - Token-level: Perplexity / cross-entropy on held-out text.
   - Task-level: a small suite (e.g., 3‚Äì5 standard evals‚ÄîARC, HellaSwag, MMLU-lite, GSM8K subset). Report accuracy and variance across seeds.
   - Scaling curves: performance vs. (a) number of members and (b) size per member (this is one of your listed research questions).

3. A concrete win
   - ex. Quality at fixed compute: An ensemble of K√ó0.5B matches or beats a single 1.5‚Äì3B student on perplexity and at least 2 downstream tasks.
   - ex. Latency/throughput on commodity hardware: Parallel ensemble on K devices achieves competitive latency and higher accuracy than a single small model.
   - ex. Specialization advantage: Ensembles trained with specialization (routing or clustered data) outperform same-size, non-specialized ensembles.
   - ex. Framework for distillation + routings.

Week 1: Method Exploration ‚Äî What Works and What Doesn‚Äôt

Goal: Identify which distillation + ensembling methods ac tually improve performance. Prioritize method-level comparisons over fine-tuning.

üîπ1.1 ‚Äì Setup for Apples-to-Apples Comparison
   [X] Fix dataset splits and logging
   [X] Generate new teacher logits with ids for clustering
   [O] Regenerate the dataset with the new chat templates and ids
   [X] Re-cache the teacher logits with the ids
   [X] Implement CSV logging for all experiments
   [] Run Teacher (frozen) and Teacher (finetuned) on distillation dataset
   [] Evaluate teacher on downstream tasks (ARC, HellaSwag, MMLU-lite, GSM8K)

üîπ1.2 ‚Äì Ensemble Formation Method Comparison
   Run 2‚Äì3 members per method to reduce cost but expose differences early.
   Track: perplexity, task accuracy, ECE, variance.
   Have the following models saved:

   A. Independent Deep Ensembles
      [] Different seeds / modest data permutations (Inference via probability averaging)

   B. Specialization (CBTM-style)
      [] Finalize dataset clustering loading
      [] Train students on cluster-specific subsets (80/20)
      Inference via: uniform averaging or routing w/ learned mixture weights (KL loss)

   C. Boosting / Residual Fitting
      [] First student = generalist
      [] Next student = trained on top-q% KL-divergent examples from round r (q ‚àà {10%, 25%})
      Optional: weighted KD loss or residual loss

   D. Baseline Student
      [] Secure a single small trained model using KL or hybrid loss on full data for evaluation

   E. Raw Student (ensemble)
      [] Untrained to measure against the other methods

üîπ1.3 ‚Äì Extra methods

   A. Negative Correlation Learning (NCL)
      [] Train ensemble members to minimize both prediction error AND correlation between members
      [] Use Œª-penalty term: Loss = MSE + Œª * correlation_penalty
      [] Forces diversity while maintaining accuracy
      [] Inference: Simple averaging (no routing needed)

   B. Bagging with Random Data Subsets
      [] Each student trained on 60-80% random sample of full dataset (with replacement)
      [] Compare against clustering-based specialization (Method B)
      [] Less structured than CBTM but potentially more robust

   C. Synthetic dataset
      [] Evaluate training on synthetic dataset

üîπ1.4 ‚Äì Evaluate; Triage output:
   [] Evaluate models trained on different methods on the Olmes benchmark
      - What methods work best (on perplexity + accuracy + performance)?
      - Does specialization help (CBTM/routing)?
      - Do any methods fail (e.g. boosting doesn‚Äôt help)?

Week 2 ‚Äì Targeted Optimization + Robustness

Goal: Tune the best method(s) from Week 1. Improve performance, analyze behavior, and test generalization.

üîπ2.1 Optimization
   [] Run grid: Œ± ‚àà {0, 0.5, 1}, œÑ ‚àà {1, 2}
   Sweep for:
      [] Single student
      [] Best ensemble method(s)
   Track: dev perplexity, task score, ECE

üîπ2.2 Generalization + Robustness
   [] Evaluate:
      - OOD domain/task
      - Confidence calibration (ECE)
      - Train routing model (if CBTM specialization selected)
        (Use teacher logits to train router on cluster index or gating weights)

üîπ2.3 Ablations
   - Does routing help vs uniform averaging?
   - What‚Äôs the effect of member diversity (random init vs data cluster)?

Week 3 ‚Äì Scaling Laws + Final Story

Goal: Show why ensemble distillation matters at scale.

üîπ3.1 Scaling Experiments
   [] Fix compute budget (tokens/flops)
   Vary:
      [] Ensemble size K ‚àà {1, 2, 4, 8, 12}
      [] (Optional) member size: 0.5B vs 1B
   Plot:
      [] Perplexity vs K
      [] Accuracy vs K
      [] Calibration vs K
      [] Model count vs performance
      [] Param count vs Performance (optional)
      [] Measure latency / report on GPU usage

üîπ3.2 Final Table + Win Conditions
   - Leaderboard: each method
   - Scaling laws
   - Graphs and plots
   - ECE/OOD table
   - Highlight win

Notes:
- Boosting helps when successive students see harder or different slices (reweighting by prior errors). With limited capacity, you won‚Äôt close all residuals, but you can still gain by concentrating each new student where the ensemble is weak.
- Temperature selection: choose œÑ that minimizes dev perplexity when distilling; don‚Äôt guess. Reuse that œÑ for all comparable runs.
- Compute parity: when comparing single vs ensemble, keep total tokens/flops in the same ballpark‚Äîor report both quality and cost.

Questions:
- Hard-example mining: top-q% selection (next round's dataset gets the highest scored samples) or weighted KD (weigh per-example loss)?
- Probability averaging for each ensemble member after training?
- Variable Œ± throughout the training? (i.e. start out with pure distillation, finish with next token, or the other way around)?